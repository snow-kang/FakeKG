{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Jupyter Notebook's kernel.json, set \"env\": {\"PYTHONHASHSEED\":\"0\"} for reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import json\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "from random import seed, shuffle, randint, sample\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE: https://github.com/villmow/datasets_knowledge_embedding/blob/master/FB15k-237/entity2wikidata.json\n",
    "# with open('entity2wikidata.json', 'r') as f:\n",
    "#     fb_dict = json.load(f)\n",
    "\n",
    "%store -r fb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict(txt):\n",
    "    lines = txt.read().split(\"\\n\")\n",
    "    new_dict = {}\n",
    "    for line in lines:\n",
    "        [ID, val] = line.split(\"\\t\")\n",
    "        new_dict[ID] = val\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    if dataset == \"Umls\" or dataset == \"Nations\":\n",
    "        entities = open(dataset + \"/entities.txt\",\"r\") \n",
    "        relations = open(dataset + \"/relations.txt\", \"r\")\n",
    "        triples = open(dataset + \"/triples.txt\", \"r\")\n",
    "\n",
    "        e_dict = get_dict(entities)\n",
    "        r_dict = get_dict(relations)\n",
    "        triples = triples.read().split(\"\\n\")\n",
    "\n",
    "    elif dataset == \"FB15K\":\n",
    "        triples_temp = open(\"fb_train.txt\", \"r\").read().split(\"\\n\")\n",
    "        \n",
    "        e_dict = fb_dict\n",
    "        r_dict = []\n",
    "        triples = []\n",
    "        \n",
    "        for t in triples_temp:\n",
    "            [ent1, rel, ent2] = t.split(\"\\t\")\n",
    "            if ent1 in e_dict and ent2 in e_dict:\n",
    "                r_dict.append(rel)\n",
    "                triples.append(t)\n",
    "        \n",
    "    return e_dict, r_dict, triples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in a KG, returns its NetworkX graph and \n",
    "# a dictionary where the key is an edge pair (obj1, obj2)\n",
    "# and the value is the edge's label\n",
    "\n",
    "def construct_DiGraph(KG):\n",
    "    G = nx.DiGraph()\n",
    "    G_labels = {}\n",
    "    for t in KG:\n",
    "        triple = t.split(\"\\t\")\n",
    "        if dataset == \"Nations\" or dataset == \"Umls\":\n",
    "            rel = triple[0]\n",
    "            ent_1 = triple[1]\n",
    "            ent_2 = triple[2]\n",
    "        elif dataset == \"FB15K\":\n",
    "            ent_1 = triple[0]\n",
    "            rel = triple[1].split(\"/\")[-1]\n",
    "            ent_2 = triple[2]\n",
    "        G_labels[(ent_1, ent_2)] = rel\n",
    "        G.add_edge(ent_1, ent_2)\n",
    "        \n",
    "    return G, G_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G_0 is the original graph represented as a set of the original tab separated string lines\n",
    "# D and A are some threshold percentages (expressed as integers)\n",
    "# L is the set of possible edge label id's\n",
    "# SizeU is the desired size of U\n",
    "\n",
    "def generate_U(G0, D, A, L, SizeU):\n",
    "    \n",
    "    # To ensure that there are no identical KGs\n",
    "    U_set = set([frozenset(G0)])\n",
    "    \n",
    "    U = {}\n",
    "    U[0] = G0\n",
    "   \n",
    "    while len(U) < SizeU:\n",
    "        d = randint(0, D)\n",
    "        a = randint(0, A)\n",
    "        lst = list(U_set)\n",
    "        shuffle(lst)\n",
    "        G_prime = set(lst[0])\n",
    "        G_prime_graph, _ = construct_DiGraph(G_prime)\n",
    "        G_prime_size = len(G_prime)\n",
    "        G_prime_nodes = G_prime_graph.nodes()\n",
    "        \n",
    "        num_edges_to_delete = int(round(d/100*G_prime_size))\n",
    "        num_edges_to_add = int(round(a/100*G_prime_size))\n",
    "        \n",
    "        # delete d% of edges in G'\n",
    "        lst = list(G_prime)\n",
    "        shuffle(lst)\n",
    "        G_prime = set(lst[:G_prime_size-num_edges_to_delete])\n",
    "        \n",
    "        # add a% of edges to G'\n",
    "        for _ in range(num_edges_to_add):\n",
    "            new_edge = sample(G_prime, 1)[0] # initialize to a known edge \n",
    "            while new_edge in G_prime:\n",
    "                lst = list(L)\n",
    "                shuffle(lst)\n",
    "                new_label = lst[0]\n",
    "                lst = list(e_dict)\n",
    "                shuffle(lst)\n",
    "                new_endpoints = lst[:2]\n",
    "                if dataset == \"Nations\" or dataset == \"Umls\":\n",
    "                    new_edge = str(new_label) + \"\\t\" + str(new_endpoints[0]) + \"\\t\" + str(new_endpoints[1])\n",
    "                elif dataset == \"FB15K\":\n",
    "                    new_edge = str(new_endpoints[0])+\"\\t\"+\"addedEdge/\" + new_label + \"\\t\" + str(new_endpoints[1])\n",
    "                \n",
    "            G_prime.add(new_edge)\n",
    "            \n",
    "        G_prime_size = len(G_prime)\n",
    "        \n",
    "        if frozenset(G_prime) not in U_set and G_prime_size >= 8 and G_prime_size <=12:\n",
    "            U[len(U)] = G_prime\n",
    "            U_set.add(frozenset(G_prime))\n",
    "    \n",
    "    return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns distance graph GI of KGs\n",
    "\n",
    "def generate_GI(U, d, t):\n",
    "    G_I = nx.Graph()\n",
    "    # Add edge if distance between KGs falls within range t\n",
    "    for i in range(len(U)):\n",
    "        for j in range(i+1, len(U)):\n",
    "            dist = d(U[i], U[j])\n",
    "            if dist <= t[1] and dist >= t[0]:\n",
    "                G_I.add_edge(i, j)\n",
    "    return G_I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacardian_dist(KG_a, KG_b): \n",
    "    KG_intersection = len(KG_a.intersection(KG_b))\n",
    "    KG_union = len(KG_a.union(KG_b))\n",
    "    if (KG_union == 0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 1-(KG_intersection/KG_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clique_computation(G_I, K_0, n):\n",
    "    G = G_I\n",
    "    C = []\n",
    "    C_nodes = set()\n",
    "    C_graph = nx.DiGraph()\n",
    "\n",
    "    while nx.graph_clique_number(G) > n:\n",
    "        all_max_cliques = list(filter(lambda x: len(x) > n, list(nx.find_cliques(G))))\n",
    "        shuffle(all_max_cliques)\n",
    "        max_clique = set(all_max_cliques[0])\n",
    "        \n",
    "        # Construct max_clique_graph by finding the subgraph of G with max_clique's nodes\n",
    "        max_clique_graph = G.subgraph(max_clique)\n",
    "        \n",
    "        # Add this max_clique to C\n",
    "        C.append(max_clique)\n",
    "\n",
    "        # Adds the nodes of max_clique to C_nodes\n",
    "        C_nodes.update(max_clique)\n",
    "        \n",
    "        # Adds max_clique_graph to C_graph\n",
    "        C_graph = nx.algorithms.operators.binary.compose(C_graph, max_clique_graph) \n",
    "        \n",
    "        if K_0 in max_clique:\n",
    "            return C\n",
    "        \n",
    "        G.remove_nodes_from(C_nodes)\n",
    "        \n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clique_fakeKG(G_I, K_0, n):\n",
    "    C = clique_computation(G_I, K_0, n)\n",
    "    \n",
    "    shuffle(C)\n",
    "    for clique in C:\n",
    "        if K_0 in clique:\n",
    "            clique.remove(K_0)\n",
    "            lst = list(clique)\n",
    "            shuffle(lst)\n",
    "            K = set(lst[:n])\n",
    "            K.add(K_0)\n",
    "            return K\n",
    "        \n",
    "    return set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Test Fakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter misleading data for Nations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nation_data(triples):\n",
    "    # ids of triples that are not misleading\n",
    "    nation_rels = list(chain(range(0,12), range(162, 242), range(255, 283), \\\n",
    "                              range(590, 613),range(677, 691), range(703, 735)))\n",
    "\n",
    "    nation_triples = list(map(lambda x: triples[x], nation_rels))\n",
    "\n",
    "    # ids of relationships that are not misleading\n",
    "    nation_rels = [0,5,6,8,27,33,35]\n",
    "    return nation_triples, nation_rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fakes(i, D, A, k, U_size):\n",
    "    global e_dict, r_dict, triples, time_taken\n",
    "    seed(i)              \n",
    "    e_dict, r_dict, triples = load_data()\n",
    "\n",
    "    if dataset == \"Nations\":\n",
    "        nation_triples, nation_rels = get_nation_data(triples)\n",
    "        G_0 = set()\n",
    "        shuffle(nation_triples)\n",
    "        G_0.update(nation_triples[:k])\n",
    "        U = generate_U(G_0, D, A, nation_rels, U_size)\n",
    "    elif dataset == \"Umls\":\n",
    "        G_0 = set()\n",
    "        shuffle(triples)\n",
    "        G_0.update(triples[:k])\n",
    "        U = generate_U(G_0, D, A, set(range(len(r_dict))), U_size)\n",
    "    elif dataset == \"FB15K\":\n",
    "        G_0 = set()\n",
    "        shuffle(triples)\n",
    "        G_0.update(triples[:k])\n",
    "        U = generate_U(G_0, D, A, r_dict, U_size)\n",
    "    \n",
    "    d = jacardian_dist\n",
    "    K_0 = 0\n",
    "    n = 3\n",
    "\n",
    "    t_intervals = [[0,.33],[0.33,0.66], [.66,1]]\n",
    "    KGs = []\n",
    "    intervals_dict = {}\n",
    "    fails = []\n",
    "\n",
    "    for t in t_intervals:\n",
    "        G_I = generate_GI(U, d, t)\n",
    "        result = clique_fakeKG(G_I, K_0, n)\n",
    "        if len(result) == 0:\n",
    "            fails.append(\"fail\")\n",
    "            break\n",
    "        else:\n",
    "            KGs.append(result)\n",
    "            intervals_dict[str(t)] = result\n",
    "            \n",
    "    flat_KGs = list(set([item for KG in KGs for item in KG]))\n",
    "    shuffle(flat_KGs)\n",
    "\n",
    "    return U, flat_KGs, intervals_dict, fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding tests that contain 3 fake graphs per interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_tests(dataset):\n",
    "    global results_txt\n",
    "    U_size = 50\n",
    "\n",
    "    all_tests = []\n",
    "    for i in range(0,22):\n",
    "        results_txt += \"\\nSeed: {}\\n\".format(i)\n",
    "        print(\"i:\", i)\n",
    "        valid_tests = []\n",
    "        for D in range(20,60,5):\n",
    "            for A in range(20, 60, 5):\n",
    "                for k in range(8,13):\n",
    "                    U, flat_KGs, intervals_dict, fails = generate_fakes(i, D, A, k, U_size)\n",
    "                    if len(fails) == 0:\n",
    "                        valid_tests.append([i, D, A, k, U_size])\n",
    "                        break\n",
    "\n",
    "        results_txt += str(valid_tests) + \"\\n\"\n",
    "        shuffle(valid_tests)\n",
    "        all_tests.append(valid_tests[0])\n",
    "            \n",
    "    return all_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run single test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(U, flat_KGs, intervals_dict, testnumber):\n",
    "    summary_filename = \"FakeKGTests/\" + dataset + \"/Test\" + str(testnumber) + \"/summary.txt\"\n",
    "\n",
    "    txt = \"Dataset: {}\\n\".format(dataset)\n",
    "    txt += \"\\nTest number: {}\\n\".format(testnumber)\n",
    "\n",
    "    txt += \"\\nOrder of graphs in test:\\n\"\n",
    "    for idx in range(len(flat_KGs)):\n",
    "        txt += \"Graph {}: KG {}\\n\".format(idx+1, flat_KGs[idx])\n",
    "\n",
    "    summary_lines = [txt,\n",
    "                     \"\\nParameters:\\n\", \"i is {}, D is {}, A is {}, k is {}, U_size is {}\\n\"\\\n",
    "                              .format(i, D, A, k, U_size), \"\\nIntervals:\\n\"]\n",
    "    for idx in intervals_dict:\n",
    "        summary_lines.append(idx + \": \" + str(intervals_dict[idx]) + \"\\n\")\n",
    "\n",
    "    for idx1 in range(len(flat_KGs)):\n",
    "        txt = \"\\nPairwise distances for Graph {} (KG {}):\\n\".format(idx1+1, flat_KGs[idx1]) \n",
    "        for idx2 in range(len(flat_KGs)):\n",
    "            txt += \"from Graph {} (KG {}): {}\\n\".format(idx2+1, flat_KGs[idx2], \\\n",
    "                                                        jacardian_dist(U[flat_KGs[idx1]], U[flat_KGs[idx2]]))\n",
    "        summary_lines.append(txt)\n",
    "\n",
    "    summary_file = open(summary_filename,\"w\") \n",
    "    summary_file.writelines(summary_lines)\n",
    "    summary_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_JSON(U, flat_KGs, testnumber):\n",
    "    for a in range(1, len(flat_KGs)+1):\n",
    "        KG_id = flat_KGs[a-1]\n",
    "        json_links = []\n",
    "        json_nodes = []\n",
    "\n",
    "        G, G_labels = construct_DiGraph(U[KG_id]) \n",
    "        for node in G.nodes():\n",
    "\n",
    "            if dataset == \"Nations\" or dataset == \"Umls\":\n",
    "                new_node = {\n",
    "                    \"name\": e_dict[node],\n",
    "                    \"id\": node\n",
    "                }\n",
    "            elif dataset == \"FB15K\":\n",
    "                new_node = {\n",
    "                    \"name\": e_dict[node][\"label\"],\n",
    "                    \"id\": node\n",
    "                }\n",
    "            json_nodes.append(new_node)\n",
    "\n",
    "        for edge in G.edges():\n",
    "            if dataset == \"Nations\" or dataset == \"Umls\":\n",
    "                new_link = {\n",
    "                    \"source\": edge[0],\n",
    "                    \"target\": edge[1],\n",
    "                    \"type\": r_dict[G_labels[(edge[0], edge[1])]]\n",
    "                }\n",
    "            elif dataset == \"FB15K\":\n",
    "                new_link = {\n",
    "                    \"source\": edge[0],\n",
    "                    \"target\": edge[1],\n",
    "                    \"type\": G_labels[(edge[0], edge[1])].split(\"/\")[-1]\n",
    "                }\n",
    "            json_links.append(new_link)\n",
    "\n",
    "        json_dict = {\n",
    "            \"nodes\": json_nodes,\n",
    "            \"links\": json_links\n",
    "        }\n",
    "\n",
    "        graph_filename = \"FakeKGTests/\" + dataset + \"/Test\" + str(testnumber) + \"/Graph\" + str(a) + \".json\" \n",
    "\n",
    "        with open(graph_filename, 'w') as json_file:\n",
    "          json.dump(json_dict, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions():\n",
    "    questions_filename = \"FakeKGTests/\" + dataset + \"/questions.txt\"\n",
    "    \n",
    "    twentytwo_tests = all_tests\n",
    "    shuffle(twentytwo_tests)\n",
    "    five_tests = twentytwo_tests[:5]\n",
    "\n",
    "    questions_lines = \"\"\n",
    "    for test in five_tests:\n",
    "        [i, D, A, k, U_size] = test\n",
    "        U, flat_KGs, intervals_dict, fails = generate_fakes(i, D, A, k, U_size)\n",
    "        original_graph = list(U[0])\n",
    "        shuffle(original_graph)\n",
    "        \n",
    "        if dataset == \"FB15K\":\n",
    "            [obj1, rel, obj2] = original_graph[0].split(\"\\t\")\n",
    "            rel = rel.split(\"/\")[-1]\n",
    "            obj1 = e_dict[obj1][\"label\"]\n",
    "            obj2 = e_dict[obj2][\"label\"]\n",
    "        else:\n",
    "            [rel, obj1, obj2] = original_graph[0].split(\"\\t\")\n",
    "            rel = r_dict[rel]\n",
    "            obj1 = e_dict[obj1]\n",
    "            obj2 = e_dict[obj2]\n",
    "                            \n",
    "        questions_lines += obj1 + \" and \" + obj2 + \" are related by \" + rel + \"\\n\"\n",
    "        \n",
    "        if dataset == \"Nations\":\n",
    "            nation_triples, nation_rels = get_nation_data(triples)\n",
    "            lst = list(nation_rels)\n",
    "        else:\n",
    "            lst = list(r_dict)\n",
    "            \n",
    "        shuffle(lst)\n",
    "        answer_choices = set([rel])\n",
    "\n",
    "        while len(answer_choices) < 5:\n",
    "            new_choice = lst.pop()\n",
    "            if dataset == \"FB15K\":\n",
    "                answer_choices.add(new_choice.split(\"/\")[-1])\n",
    "            elif dataset == \"Nations\":\n",
    "                answer_choices.add(r_dict[str(new_choice)])\n",
    "            else:\n",
    "                answer_choices.add(r_dict[new_choice])\n",
    "\n",
    "        answer_choices = list(answer_choices)\n",
    "        shuffle(answer_choices)\n",
    "        for choice in answer_choices:\n",
    "            questions_lines += choice + \"\\n\"\n",
    "            \n",
    "        questions_lines += \"\\n\"\n",
    "        \n",
    "    questions_file = open(questions_filename,\"w\") \n",
    "    questions_file.writelines(questions_lines)\n",
    "    questions_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_csv():\n",
    "    csv_filename = \"FakeKGTests/\" + dataset + \"summary.csv\"  \n",
    "    csv_lines = \"\"\n",
    "\n",
    "    for test in all_tests:\n",
    "        [i, D, A, k, U_size] = test\n",
    "        U, flat_KGs, intervals_dict, fails = generate_fakes(i, D, A, k, U_size)\n",
    "\n",
    "        for KG in flat_KGs:\n",
    "            csv_lines += str(round(jacardian_dist(U[KG], U[0]), 2)) + \",\"\n",
    "        csv_lines += \"\\n\"\n",
    "    \n",
    "    csv_file = open(csv_filename,\"w\") \n",
    "    csv_file.writelines(csv_lines)\n",
    "    csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0\n",
      "i: 1\n",
      "i: 2\n",
      "i: 3\n",
      "i: 4\n",
      "i: 5\n",
      "i: 6\n",
      "i: 7\n",
      "i: 8\n",
      "i: 9\n",
      "i: 10\n",
      "i: 11\n",
      "i: 12\n",
      "i: 13\n",
      "i: 14\n",
      "i: 15\n",
      "i: 16\n",
      "i: 17\n",
      "i: 18\n",
      "i: 19\n",
      "i: 20\n",
      "i: 21\n"
     ]
    }
   ],
   "source": [
    "dataset = \"FB15K\"\n",
    "\n",
    "results_filename = \"AllResults/\" + dataset + \"Results.txt\"\n",
    "results_file = open(results_filename,\"w\") \n",
    "results_txt = \"Dataset: {}\\n\".format(dataset)\n",
    "\n",
    "all_tests = generate_tests(dataset)\n",
    "\n",
    "testnumber = 1\n",
    "for test in all_tests:\n",
    "    [i, D, A, k, U_size] = test\n",
    "    U, flat_KGs, intervals_dict, fails = generate_fakes(i, D, A, k, U_size)\n",
    "    generate_summary(U, flat_KGs, intervals_dict, testnumber)\n",
    "    generate_JSON(U, flat_KGs, testnumber)\n",
    "    testnumber += 1\n",
    "\n",
    "results_file.writelines(results_txt)\n",
    "results_file.close()\n",
    "    \n",
    "generate_questions()\n",
    "generate_csv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
